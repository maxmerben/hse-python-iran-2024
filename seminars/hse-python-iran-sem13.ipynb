{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea17432-ea86-4cb1-bed1-bdd795b70aa0",
   "metadata": {},
   "source": [
    "# Семинар 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8401e33-1c33-4067-a729-22c174c893ca",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784e03a-3359-4122-93db-95072e271ea3",
   "metadata": {},
   "source": [
    "Токенизация — не такая тривиальная задача, как кажется на первый взгляд. Например, как токенизировать выражение «`Hello world!`»? Можно придумать много разных способов:\n",
    "1) `Hello` — `world`\n",
    "2) `Hello` — `world!`\n",
    "3) `Hello` — `world` — `!`\n",
    "4) `Hello` — ` ` — `world` — `!`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414716d0-0856-47bf-a16c-d8f54f321137",
   "metadata": {},
   "source": [
    "Реализуем на питоне регулярное выражение, которое будет токенизировать текст по формату (4) выше, то есть раздельно сохраняя все последовательности букв, знаков препинания и пробелов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d9acd7-9a2a-4b64-a35b-c0e64ba87f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', '!!!', ' ', 'You', ' ', 'are', ' ', 'so', ' ', 'beautiful', ',', ' ', 'isn', '’', 't', ' ', 'it', ' ', 'amazing', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello world!!! You are so beautiful, isn’t it amazing!\"\n",
    "print(re.findall(\"[A-Za-z]+| +|[\\.,’!\\?]+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88aac4-ae25-4d82-8503-8597eb6da2eb",
   "metadata": {},
   "source": [
    "Получилось! Но, как видите, токенизировать регулярками — не самое лёгкое занятие. Для реального текста нам бы пришлось прописывать ещё миллион отдельных случаев (все знаки препинания, цифры, всякие прочие символы и разные другие проблемы), и регулярное выражение стало бы огромным. Иногда, когда мы хотим настроить в алгоритме токенизации какие-то тонкие детали, у нас просто нет другого выхода. Но для большинства случаев при работе с крупными языками люди пользуются уже готовыми решениями. Одно из них — токенизаторы из библиотеки `nltk`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad267cf-272c-441e-8953-0f5fa668c0b0",
   "metadata": {},
   "source": [
    "### Библиотека `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728fbeff-c315-41c5-8b31-1168bf7eb257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Установка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743fba8-8081-4fb5-b0d2-4749fa899ef0",
   "metadata": {},
   "source": [
    "Итак, **`nltk`** — библиотека, то есть такой пакет с пакетами, мега-модуль, содержащий другие модули. Попробуем различные токенизаторы из этой библиотеки (см. [документацию](https://www.nltk.org/index.html) `nltk`). Сначала нужно её импортировать, а ещё докачать специальные файлы, необходимые для работы некоторых инструментов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45609081-4455-43d9-b678-2ab7a611cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bdcf076-64cb-4bdc-9e0a-97900c4f9c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Samsung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Samsung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")   # <-- это нужно скачать для токенизации\n",
    "nltk.download(\"stopwords\")   # <-- это нужно скачать для стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df05f41-6c6a-4ae9-93f2-7438faa36415",
   "metadata": {},
   "source": [
    "Будем проверять, как работают разные токенизаторы, на вот этом английском тексте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8560ca-5778-4682-92a1-cb7ddb871120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good muffins cost $3.88  in New York. \n",
      "Please buy me two of them... Or don't!\n",
      "\n",
      "Thanks.\n"
     ]
    }
   ],
   "source": [
    "text = \"Good muffins cost $3.88  in New York. \\nPlease buy me two of them... Or don't!\\n\\nThanks.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f023c8e-033a-432c-9dc6-6b74d1c67272",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Модуль **`nltk.tokenize`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50130a-fcbe-4293-b943-37b1f262ba35",
   "metadata": {},
   "source": [
    "За токенизацию в библиотеке `nltk` отвечает вложенный в неё модуль **`nltk.tokenize`**. Самый простой и часто используемый инструмент для токенизации в нём — функция `word_tokenize()`. Заметьте, что она не просто делит по пробелам, но (1) выделяет знаки препинания, (2) игнорирует пробелы и переход на новую строку, (3) делит стяжённую словоформу *don’t* на *do* и *n’t*, (4) отделяет знак доллара, но сохраняет целиком число с десятичной запятой ($3.88$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06fda5c-864a-420f-859d-f1b8a6541a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '...', 'Or', 'do', \"n't\", '!', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80140d1-99d7-4ade-88a8-f7a29b9cba52",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef241dd-3485-475b-8d34-eacea4964c8f",
   "metadata": {},
   "source": [
    "(На всякий случай вспомним, как мы работаем с модулями. Добраться до функции `word_tokenize()` можно разными способами, вот некоторые из них с пояснениями:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8216c2de-6b63-453e-87cc-976c77f6a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Эй', ',', 'токенизируй', 'меня', ',', 'друг', '!']\n"
     ]
    }
   ],
   "source": [
    "# 1 способ\n",
    "\n",
    "# импортируем всю библиотеку nltk\n",
    "import nltk\n",
    "# используем вложенный в nltk модуль nltk.tokenize, а из него функцию word_tokenize\n",
    "print(nltk.tokenize.word_tokenize(\"Эй, токенизируй меня, друг!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177aa385-3971-4737-aa7c-40ee0e2388dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Эй', ',', 'токенизируй', 'меня', ',', 'друг', '!']\n"
     ]
    }
   ],
   "source": [
    "# 2 способ\n",
    "\n",
    "# импортируем только конкретный модуль nltk.tokenize\n",
    "import nltk.tokenize\n",
    "# используем импортированный модуль nltk.tokenize, а из него функцию word_tokenize\n",
    "print(nltk.tokenize.word_tokenize(\"Эй, токенизируй меня, друг!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05e58fd-2f3a-4ac2-9bf9-d5247f603640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Эй', ',', 'токенизируй', 'меня', ',', 'друг', '!']\n"
     ]
    }
   ],
   "source": [
    "# 3 способ\n",
    "\n",
    "# импортируем только конкретный модуль nltk.tokenize и называем её каким-нибудь псевдонимом (например, t)\n",
    "import nltk.tokenize as t\n",
    "# обращаемся к модулю nltk.tokenize с помощью псевдонима t и из него используем функцию word_tokenize\n",
    "print(t.word_tokenize(\"Эй, токенизируй меня, друг!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde45945-3066-4345-b790-6b1b4aed93d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Эй', ',', 'токенизируй', 'меня', ',', 'друг', '!']\n"
     ]
    }
   ],
   "source": [
    "# 4 способ\n",
    "\n",
    "# импортируем конкретную функцию word_tokenize из модуля nltk.tokenize\n",
    "# (импортируется только word_tokenize, другие функции будут недоступны)\n",
    "from nltk.tokenize import word_tokenize\n",
    "# зато использовать функцию word_tokenize гораздо проще\n",
    "print(word_tokenize(\"Эй, токенизируй меня, друг!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37263374-716b-47b6-a5a3-6fe37ef3c1c2",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36315de-da1e-4127-a4a7-f2964d1ee05d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Другие токенизаторы **`nltk`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1fc575-30d2-480f-95c0-fd14b2e61b44",
   "metadata": {},
   "source": [
    "В `nltk` есть и другие токенизаторы (полный список можно найти в [документации](https://www.nltk.org/index.html)). Посмотрим для примера на ещё один из них, `ToktokTokenizer`. Он используется иначе, чем мы привыкли — это не функция, а уникальный объект, то есть **объект собственного класса**.\n",
    "\n",
    "Классы — это как бы *новые типы данных*, созданные пользователями питона. Как у строк или у списков есть свои методы (`str.split()`, `str.upper()`, `list.append()`), так же и к объектам новых классов можно придумать новые методы. Но только строки — это встроенный тип данных, а `ToktokTokenizer` — это такой особый класс, созданный разработчиками `nltk`. Они прописали, какую информацию объекты этого класса будут хранить, и придумали методы, с помощью которых с этими объектами можно взаимодействовать. Про объект `ToktokTokenizer` можно думать как про специальный инструмент, который будет токенизировать для нас тексты.\n",
    "\n",
    "Сначала импортируем сам класс `ToktokTokenizer`, а потом **создадим новый объект этого класса**. Чтобы создать объект класса, нужно вызвать функцию, которая совпадает с его названием (то есть функцию `ToktokTokenizer()`). Можно сравнить этом с тем, как мы создавали новый (пустой) список функцией `list()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8687f3e-17b3-4b6a-a545-52f93bfd8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560aba46-5bf8-4d0c-a6ec-cbb5b3a2025b",
   "metadata": {},
   "source": [
    "(Обратите внимание, что `ToktokTokenizer` мы импортировали не просто из библиотеки `nltk`, и даже не из её модуля `nltk.tokenize`, а из подмодуля `nltk.tokenize.toktok`, который сам вложен в модуль `nltk.tokenize`. К сожалению, структура конкретных модулей и библиотек может быть довольно запутанной. Но есть и хорошие новости: (1) никто не ожидает, что вы это запомните наизусть, (2) никто и не помнит всё это наизусть. Люди просто гуглят или ищут в документации конкретных библиотек и модулей, как сделать то или иное действие с этими инструментами. Это абсолютно нормально и так делают все!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae592b9-08e3-470b-ba2c-aa763cba918f",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627fc0b-a62f-43bf-b6d7-cd932f907b6a",
   "metadata": {},
   "source": [
    "Итак, теперь у нас есть объект-токенизатор, заточённый в переменной `tokenizer`. Убедимся, что это правда объект отдельного типа `ToktokTokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0073abdb-d7de-4aa3-9edb-9d4312454ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tokenize.toktok.ToktokTokenizer object at 0x0000023EDC1849D0>\n",
      "<class 'nltk.tokenize.toktok.ToktokTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c55fd5-e18b-405b-b2eb-33687b2cebe1",
   "metadata": {},
   "source": [
    "У этого класса есть метод `.tokenize()`, в который можно подать текст. Обратите внимание, что результат слегка отличается от функции `word_tokenize()`, которую мы видели выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb26f8c-4844-49e0-aafa-7f5a097e077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them', '...', 'Or', 'don', \"'\", 't', '!', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac0705-c35b-41ce-a6d6-2a1b593918e8",
   "metadata": {},
   "source": [
    "(Напоследок: вообще-то записывать объект в переменную необязательно, можно просто создать объект на ходу и тут же вызвать от него метод.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b145adc-61f2-4d75-bf79-ec06f0018dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them', '...', 'Or', 'don', \"'\", 't', '!', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(ToktokTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a210df-94d2-4818-80ce-b1e29469a797",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15bf1cd-5c9c-43c4-96b3-b02faf47c084",
   "metadata": {},
   "source": [
    "В отдельном модуле `nltk.corpus` внутри библиотеки `nltk` можно найти списки **стоп-слов** (*stopwords*) для разных языков. Стоп-слова — это слова (и словоформы), которые частотны в любых текстах на определённом языке, имеют функциональное значение (союзы, предлоги, местоимения, междометия) и поэтому бесполезны при решении задач, в которых нужно понять что-то о смысле текста. Такой список стоп-слов можно собрать вручную, но `nltk` уже любезно сделал это за нас. Чтобы ими воспользоваться, нужно импортировать объект `stopwords` из `nltk.corpus`. У него есть метод `.words()`, в который можно в качестве аргумента подать название языка и получить список со стоп-словами. Такие списки есть для разных языков, включая русский:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d496c4be-9538-47be-8300-2bb5f7417e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87385d23-ed31-4a85-b196-f82c2acc087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7fc01e-0d61-480d-b0fc-5050d4dc7c33",
   "metadata": {},
   "source": [
    "К сожалению, персидский не входит в число языков, для которых `nltk` подготовил стоп-слова — но в интернете можно найти кучу подобных списков, в том числе для персидского (как в модулях, специализированных для работы с персидским, так и просто в виде списка слов на чьём-нибудь гитхабе)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a49e97a3-6804-49d2-af13-3d61f4e59d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ccc32-b05d-4bd8-a6a7-8d6173776f18",
   "metadata": {},
   "source": [
    "**Удаление стоп-слов** — стандартная процедура при предобработке текста для очень многих задач NLP. Например, когда вы токенизировали текст, можно удалить все токены, которые встречаются среди стоп-слов (в цикле поставив условие наподобие `if token in stopwords.words(\"russian\")`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13bc93-ff75-45d2-a0c0-0d55ace5781a",
   "metadata": {},
   "source": [
    "## Автоматический морфологический анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9d7a0-9f5e-4477-a939-fa2eb00c19b4",
   "metadata": {},
   "source": [
    "Что мы хотим <s>от этой жизни</s> от автоматического морфологического анализа?\n",
    "\n",
    "Вот наша троица мечты:\n",
    "1. **POS-тэггинг** (*part-of-speech tagging*, частеречный тэггинг) — чтобы компьютер сам определял часть речи и проставлял соответствующие тэги всем токенам\n",
    "2. **определение грамматических свойств** — чтобы компьютер определял для каждого токена грамматические времена, наклонения, род, число, падеж, изафет и всё прочее\n",
    "3. **лемматизация** — чтобы компьютер определял **лемму**, то есть, по-школьному, начальную форму для каждого токена\n",
    "\n",
    "Сейчас посмотрим, как это всё можно делать — для русского и для персидского."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4284c80-15d9-4f51-b11c-c4dceccba20b",
   "metadata": {},
   "source": [
    "### Русский язык"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67e678-22d4-4818-a4a9-e6c38a35cd5e",
   "metadata": {},
   "source": [
    "Вот некоторые библиотеки для работы с русским языком:\n",
    "- [**`pymorphy3`**](https://github.com/no-plagiarism/pymorphy3) — компактный и простой в использовании модуль, не самый точный и быстрый (но тоже очень хороший)\n",
    "- [`natasha`](https://natasha.github.io) — большая и мощная библиотека с кучей инструментов для русского\n",
    "- [`spacy`](https://spacy.io) — межъязыковая библиотека с моделями, натренированными решать разные NLP-задачи для разных языков, [включая русский](https://habr.com/ru/articles/531940/)\n",
    "\n",
    "Мы изучим вопрос морфологического анализа на примере **`pymorphy3`**. С его документацией можно ознакомиться **[вот здесь](https://pymorphy2.readthedocs.io/en/stable/user/guide.html#id3)** (рекомендую, это очень приятная и понятная документация!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f1ff0-a1ee-4d48-b263-5791bb304d09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Установка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd3f1e-5dfe-4eab-af43-bcd4a6f8e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymorphy3   # <- установка модуля на ваш компьютер (нужно сделать всего один раз)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e50e266-9806-4634-ba6b-2668543bbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825f0a8-70f6-4358-8646-39d23f8b65e6",
   "metadata": {},
   "source": [
    "Модуль `pymorphy3` тоже построен на *объектах*, а не на *функциях*. Самый важный инструмент в нём — это класс `MorphAnalyzer`. Это такая машинка, которая занимается морфоанализом. Инициируем объект этого класса (вызвав функцию `MorphAnalyzer()`) и запишем в переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ce2dec2-062d-4e6b-b4fc-c6157e6179cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = pymorphy3.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca0171-41a2-4e2e-b651-cc809ef738a8",
   "metadata": {},
   "source": [
    "Обратите внимание, что при создании объекта этого класса компьютер может немножечко подвисать. Это потому, что питон подгружает разные механизмы, необходимые для морфологического анализа, из файлов модуля `pymorphy3` в оперативную память вашего компьютера. Было бы нецелесообразно замусоривать оперативную память, поэтому общепринятая практика — создавать такие объекты один раз за код, сохранять их в отдельную переменную (как мы и сделали) и затем пользоваться этой переменной (а не создавать новый объект каждый раз, когда нужно что-то разобрать)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2f189-f7ad-4a4d-b489-2a4a4efbd5e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Метод **`.parse()`** и класс **`Parse`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393bf48-2c3b-4dc3-a07a-a8a8865826a1",
   "metadata": {},
   "source": [
    "Основной метод класса `MorphAnalyzer` называется **`.parse()`**. В него нужно подавать токены на русском языке (по отдельности!), и тогда наш ручной морфоанализатор будет их парсить, то есть разбирать. Посмотрим, что получится, если подать слово «стекло»:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b881502d-3927-4a58-9580-8ac70aa80d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.690476, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),)),\n",
       " Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 3),)),\n",
       " Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.023809, methods_stack=((DictionaryAnalyzer(), 'стекло', 1015, 3),))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = analyzer.parse(\"стекло\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5eb54-8392-4900-8ba3-b60e16a16583",
   "metadata": {},
   "source": [
    "Из метода `.parse()` мы получили список из объектов нового для нас класса, класса **`Parse`**, то бишь «анализ». Такие объекты содержат результат разбора. Например, первый объект в получившемся списке сообщает следующее:\n",
    "\n",
    "- токен, который в меня подали: `стекло`\n",
    "- тэги, которые я могу предложить для этого токена: существительное (`NOUN`), неодушевлённое (`inan`), среднего рода единственного числа (`neut sing`), в именительном падеже (`nomn`)\n",
    "- лемма («нормальная» форма) этой лексемы: `стекло`\n",
    "\n",
    "Однако в списке больше одного «разбора», потому что слово «стекло» неоднозначно. Оно может значить существительное «стекло» в именительном падеже (это первый разбор, с индексом 0), а может существительное «стекло» в винительном падеже (разбор с индексом 1), а ещё может форму прошедшего времени глагола «стечь» (разбор с индексом 2). Число `score` в получившихся разборах отражает вероятность, что данный анализ является верным. Как видите, наш анализатор посчитал, что «стекло» в именительном падеже — наиболее вероятный сценарий, и присвоил ему вероятность в $69$%, а такая форма глагола «стечь» встречается гораздо реже, поэтому её вероятность всего $2$%. Разборы в списке упорядочиваются по убыванию вероятности, так что, если нужен наиболее вероятный разбор из возможных, можно всегда брать первый."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f57fa-2358-46ce-9b05-e90212f62ccc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Атрибуты **`Parse`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db53089-588a-4205-9995-e97af6c7540d",
   "metadata": {},
   "source": [
    "У объекта типа `Parse` есть несколько атрибутов: тэги, лемма, вероятность и прочее. Термином «**атрибуты**» в питоне обозначаются какие-то свойства объекта, которые из него можно достать. Обращаться ним надо так же, как к методам класса, только с методами мы (как с функциями) пишем круглые скобки, а с атрибутами нет. (**[Вот здесь](https://stackoverflow.com/questions/46312470/difference-between-methods-and-attributes-in-python)** отличное объяснение разницы между атрибутами и функциями, очень рекомендую.)\n",
    "\n",
    "Например, проанализируем слово «ласковых», возьмём его первый (самый вероятный) разбор и достанем из него лемму с помощью атрибута **`.normal_form`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e52c6ba-985e-4cc9-96f8-48b175e5bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='ласковых', tag=OpencorporaTag('ADJF,Qual plur,gent'), normal_form='ласковый', score=0.5, methods_stack=((DictionaryAnalyzer(), 'ласковых', 249, 21),))\n",
      "Лемма: ласковый\n"
     ]
    }
   ],
   "source": [
    "result = analyzer.parse(\"ласковых\")[0]   # не глядя берём разбор с индексом 0\n",
    "print(result)  # выведем весь разбор\n",
    "\n",
    "print(\"Лемма:\", result.normal_form)  # выведем только лемму"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd14fa-31d2-44c3-b718-8848e841df9c",
   "metadata": {},
   "source": [
    "А вот ещё один атрибут, **`.tag`**, список тэгов-грамматических свойств токена. Здесь уже чуть более непонятные тэги: `ADJF` — прилагательное в полной форме (*ADJective*+*Full*), `Qual` — если я правильно понимаю, качественное прилагательное (*Qualitative*), `plur` — множественное число, `gent` — родительный падеж (генитив).\n",
    "\n",
    "Первый тэг, записанный прописными буквами, — всегда часть речи, остальные — прочие свойства. В `pymorphy3` используются тэги из проекта [OpenCorpora](https://www.opencorpora.org), (почти) полный список этих тэгов можно найти **[здесь](https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html#grammeme-docs)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6b3f811-ef46-47a3-a621-99fc46a0b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJF,Qual plur,gent\n"
     ]
    }
   ],
   "source": [
    "tag = result.tag\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486160d-caf0-492b-8431-02394e2ec1ce",
   "metadata": {},
   "source": [
    "Может показаться, что атрибут `.tag` выдаёт просто обычную строку. Результат действительно похож на строку по многим свойствам: например, с помощью `in` можно проверить, является ли этот токен словом определённой части речи, стоит ли он в определённом падеже, времени и так далее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c07f6eb-908f-446a-975e-fcbbe6757877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"gent\" in tag)\n",
    "print(\"ADJF\" in tag)\n",
    "\n",
    "print(\"nomn\" in tag)\n",
    "print(\"NOUN\" in tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b5da6-7e04-4212-9a0e-2ff9a3345b49",
   "metadata": {},
   "source": [
    "Но на самом деле это не просто строка, а… объект ещё одного отдельного класса. (Да, так устроены все большие модули в питоне, и это норма!) Убедимся в этом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "620ad606-bb01-4d46-ac13-1c1c9ff06bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pymorphy3.tagset.OpencorporaTag'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61be6f1-3c53-438b-b6be-29dfa44e3eff",
   "metadata": {},
   "source": [
    "В том, что это не просто строка, есть несколько плюсов. Во-первых, если спросить, нет ли в этом разборе какого-то тэга, который отсутствует в наборе тэгов OpenCorpora, `pymorphy3` это заметит и скажет вам. Допустим, вы хотели найти в тексте все слова в родительном падеже, но забыли тэг для него и написали что-то не то. Модуль не выдаст ответа и вместо этого укажет на ошибку — мол, граммемы такой не знаю, ты ошибся:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18151f03-7d3e-4ba8-a31e-c79f6960ed3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammeme is unknown: geni",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeni\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymorphy3\\tagset.py:341\u001b[0m, in \u001b[0;36mOpencorporaTag.__contains__\u001b[1;34m(self, grammeme)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammeme_is_known(grammeme):\n\u001b[1;32m--> 341\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrammeme is unknown: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrammeme\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Grammeme is unknown: geni"
     ]
    }
   ],
   "source": [
    "print(\"geni\" in tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c4156-6f98-4d39-be02-126fc474a104",
   "metadata": {},
   "source": [
    "Во-вторых, если нужно найти в цепочке тэгов сразу несколько тэгов (например, проверить токен на число и падеж одновременно), это можно сделать с помощью множества (помните такой тип данных?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a78ba56-b5b4-4e22-a49f-37223fbe51bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"gent\" in tag)\n",
    "print(\"plur\" in tag)\n",
    "\n",
    "print({\"gent\",\"plur\"} in tag)   # верно ли, что слово и в генитиве, и в мн. ч.? — верно\n",
    "print({\"gent\",\"sing\"} in tag)   # верно ли, что слово и в генитиве, и в ед. ч.? — неверно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe2c65-8f52-43a1-885b-21be64b7e15f",
   "metadata": {},
   "source": [
    "В-третьих, у этого объекта у самого есть атрибуты — с их помощью можно достать отдельные тэги, такие как часть речи, падеж, число и так далее (и вот это уже обычные строки!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f6afd27-2b5d-437b-ac35-1135d2996fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJF\n",
      "plur\n",
      "gent\n"
     ]
    }
   ],
   "source": [
    "print(tag.POS)\n",
    "print(tag.number)\n",
    "print(tag.case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443a6e5-58d8-47c3-83b8-e8c0107c8910",
   "metadata": {},
   "source": [
    "Заметьте, что если мы разобрали существительное, признака «грамматическое время» у него, конечно, не будет, но атрибут такой всё равно есть, просто выдаёт он объект `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71d83b38-4c03-46e0-95c2-e603e98f23d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tag.tense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c911c2-19f4-4aef-92ad-b07f300f39e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Упражнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fd82d-a259-46c2-84c7-6926d979bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Довольно любопытно, неужели только меня возмущают смещенные акценты \n",
    "в угоду малозначительной мишуры? Где затерялся тот первозданный минимализм \n",
    "и первоклассная стабильность, оптимизация? Уже молчу о подгрузке контента \n",
    "и качестве звонков ― это нечто лежащее за пределами моего понимания. \n",
    "И как бы то прискорбно не прозвучало, но вынужден констатировать: \n",
    "текущие метаморфозы телеги ― это первосортный кринж в чистом виде. \n",
    "По крайней мере, на мой скромный взгляд через ретроспективу. \n",
    "Ощущение, будто в одноклассники заглянул, бррррр… Сам бы не поверил \n",
    "собственным словам пару-тройку лет назад, но даже whatsapp выглядит \n",
    "более выигрышно в этой ситуации. А вообще, пожалуй, начну \n",
    "планомерно мигрировать в сигнал.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf14b9-7ea1-47ea-98ca-2f90107af800",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06907d31-45ea-41f8-b228-369156e71d0f",
   "metadata": {},
   "source": [
    "**Упражнение 1**. Токенизируйте этот текст с помощью функции `word_tokenize()` из модуля `nltk.tokenize`, результат сохраните в список. (Обязательно пропишите корректный импорт для модуля / функции, чтобы ячейку с вашим решением можно было запускать отдельно от остальной тетрадки.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57401164-ec7e-43d1-8326-ed145763c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваше решение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c6004-23c6-412d-b199-6c577d513323",
   "metadata": {},
   "source": [
    "**Упражнение 2**. Импортируйте и создайте новый объект класса `MorphAnalyzer` из `pymorphy3` (чтобы эту ячейку можно было запустить автономно, как и предыдущую). С помощью этого анализатора найдите в тексте все наречия (используйте только наиболее вероятный разбор из предлагаемых). Выведите их на экран (каждое с новой строки или списком, как вам удобнее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df7ef6-de11-401f-bd1f-329a6faaba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваше решение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb0a9a-b512-400e-8794-224ef0417052",
   "metadata": {},
   "source": [
    "**Упражнение 3**. Используя уже созданный в предыдущем упражнении объект-анализатор, найдите в тексте все имена существительные в единственном числе и родительном падеже. Для каждого слова выведите на экран через пробел (1) лемму слова и (2) его тэги (например, `понимание NOUN,inan,neut sing,gent`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0cc6f-4009-4fd9-aeb1-6a43f4d26623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваше решение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30887bf9-e068-4b79-b8b6-dfc23325330a",
   "metadata": {},
   "source": [
    "### Персидский язык"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3ed1d-4659-4614-ac19-c1e8e48a6f7e",
   "metadata": {},
   "source": [
    "Вот некоторые библиотеки для работы с персидским языком:\n",
    "- [**`hazm`**](https://github.com/roshan-research/hazm) — самый простой в использовании, неплохой по качеству модуль для стандартных задач обработки текста (токенизация, лемматизация, POS-тэггинг, анализ синтаксических зависимостей, эмбеддинги и прочие)\n",
    "- [`DadmaTools`](https://github.com/Dadmatech/DadmaTools) — бизнес-ориентированный мощный модуль для NLP-задач (токенизация, лемматизация, POS-тэггинг, морфологический анализ, анализ синтаксических зависимостей, эмбеддинги и прочие)\n",
    "- [`persian_phonemizer`](https://github.com/de-mh/persian_phonemizer/tree/main) — модуль для расстановки огласовок / фонематической транскрипции персидского текста\n",
    "\n",
    "Задачу морфологического разбора рассмотрим на примере `hazm`. (У этого модуля есть [документация](https://www.roshan-ai.ir/hazm/), но так как он писался иранцами, то и документация на персидском :) Но **[вот здесь](https://github.com/roshan-research/hazm?tab=readme-ov-file#usage)** есть некоторые англоязычные примеры.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ce146-09c6-4a5c-a5c3-9e7d229f8515",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Установка **`hazm`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90f5d9-33a3-4128-8bd6-7b97a1b2b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hazm   # <- установка модуля на ваш компьютер (нужно сделать всего один раз)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08174f7-4628-40b5-97cf-d27a2787cf78",
   "metadata": {},
   "source": [
    "Как и `pymorphy3`, модуль `hazm` тоже построен на объектах — сейчас мы посмотрим на них поподробнее. Сначала импортируем сам модуль и добавим небольшой текст для проверки его работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ecac6e0-377e-4989-ba7e-b611c4679ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d05c0322-f8f5-4cda-8d4c-88f759ed16ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "او در کودکی به پدر خود کمک می کرد و غذای فرزندان وزیر را برایشان به مدرسه می برد\n"
     ]
    }
   ],
   "source": [
    "text = \"او در کودکی به پدر خود کمک می کرد و غذای فرزندان وزیر را برایشان به مدرسه می برد\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362c18d-c6d2-4086-a523-07f7b44b35f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Нормализация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce07f3-fdde-42ab-84b4-710c63a57d90",
   "metadata": {},
   "source": [
    "Класс `hazm.Normalizer` нужен для **нормализации** текста. Это отдельная задача предобработки текста, актуальная для языков с большой вариативностью в записи слов. Создадим объект этого класса и воспользуемся его методом `.normalize()`.\n",
    "\n",
    "______\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \r",
    "<b><i>Сравните текст до нормализации и после. Что поменялось?</i></b> \r\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ec0edaa-7fb3-4260-863d-fab1b988cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_normalizer = hazm.Normalizer()   # создаём переменную и засовываем в неё наш «нормализатор»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37220e51-d787-4836-a240-b35c92f1f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_normalized = my_normalizer.normalize(text)   # применяем метод .normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8523efa4-51f6-48d3-9661-c71d6d681186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "او در کودکی به پدر خود کمک می کرد و غذای فرزندان وزیر را برایشان به مدرسه می برد\n",
      "او در کودکی به پدر خود کمک می‌کرد و غذای فرزندان وزیر را برایشان به مدرسه می‌برد\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(text_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a1c7e-15e1-4d38-a587-3af803dbda3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93629d5c-326b-48fa-a3ee-0c3a5dc1aed2",
   "metadata": {},
   "source": [
    "Токенизация в `hazm`, неожиданно, реализована не через отдельный класс, а через функцию. Функция называется `word_tokenize()`. Применим её к тексту (только уже к нормализованному!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef7fb8fc-7692-4372-87f4-7fd8189b14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = hazm.word_tokenize(text_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b700b014-a683-4375-bb72-35b4da8ffc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['او', 'در', 'کودکی', 'به', 'پدر', 'خود', 'کمک', 'می\\u200cکرد', 'و', 'غذای', 'فرزندان', 'وزیر', 'را', 'برایشان', 'به', 'مدرسه', 'می\\u200cبرد']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec677438-d739-4000-aa45-cee74a712593",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411999d-92ea-41fe-8cd6-a4c137e1aac4",
   "metadata": {},
   "source": [
    "Лемматизация в `hazm` устроена несколько громоздко. По сути она разделена на два отдельных инструмента:\n",
    "- класс **`hazm.Stemmer`** выделяет «основу» у всех слов, кроме глаголов (с помощью метода `.stem()`)\n",
    "- класс **`hazm.Lemmatizer`** выделяет «лемму» у глаголов (с помощью метода `.lemmatize()`)\n",
    "\n",
    "Я бы рекомендовал вам для полноценного разбора текста пользоваться обоими этими инструментами: сначала прогнать все токены через один, потом через другой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a894db6-5330-41a9-8a93-92051934fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stemmer = hazm.Stemmer()\n",
    "my_lemmatizer = hazm.Lemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152037b2-c8cc-417f-a397-2bf294aa2ded",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b><i>Изучите выдачу стеммера и лемматизатора. Какие слова они обработали не идеально?</i></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87dfadb2-6327-4713-a625-e1f843f2d849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:      او\n",
      "Lemmatized: او\n",
      "Stemmed:    او\n",
      "\n",
      "Token:      در\n",
      "Lemmatized: در\n",
      "Stemmed:    در\n",
      "\n",
      "Token:      کودکی\n",
      "Lemmatized: کودکی\n",
      "Stemmed:    کودک\n",
      "\n",
      "Token:      به\n",
      "Lemmatized: به\n",
      "Stemmed:    به\n",
      "\n",
      "Token:      پدر\n",
      "Lemmatized: پدر\n",
      "Stemmed:    پدر\n",
      "\n",
      "Token:      خود\n",
      "Lemmatized: خود\n",
      "Stemmed:    خود\n",
      "\n",
      "Token:      کمک\n",
      "Lemmatized: کمک\n",
      "Stemmed:    کمک\n",
      "\n",
      "Token:      می‌کرد\n",
      "Lemmatized: کرد#کن\n",
      "Stemmed:    می‌کرد\n",
      "\n",
      "Token:      و\n",
      "Lemmatized: و\n",
      "Stemmed:    و\n",
      "\n",
      "Token:      غذای\n",
      "Lemmatized: غذای\n",
      "Stemmed:    غذا\n",
      "\n",
      "Token:      فرزندان\n",
      "Lemmatized: فرزندان\n",
      "Stemmed:    فرزند\n",
      "\n",
      "Token:      وزیر\n",
      "Lemmatized: وزیر\n",
      "Stemmed:    وزیر\n",
      "\n",
      "Token:      را\n",
      "Lemmatized: را\n",
      "Stemmed:    را\n",
      "\n",
      "Token:      برایشان\n",
      "Lemmatized: برایشان\n",
      "Stemmed:    برا\n",
      "\n",
      "Token:      به\n",
      "Lemmatized: به\n",
      "Stemmed:    به\n",
      "\n",
      "Token:      مدرسه\n",
      "Lemmatized: مدرسه\n",
      "Stemmed:    مدرسه\n",
      "\n",
      "Token:      می‌برد\n",
      "Lemmatized: برد#بر\n",
      "Stemmed:    می‌برد\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens)):\n",
    "    print(\"Token:     \", tokens[i])\n",
    "    print(\"Lemmatized:\", my_lemmatizer.lemmatize(tokens[i]))\n",
    "    print(\"Stemmed:   \", my_stemmer.stem(tokens[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4734e0f-503f-4bfe-9d02-a1198e628c0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### POS-тэггинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57056d-8df0-4748-9897-057dbd3466f2",
   "metadata": {},
   "source": [
    "К сожалению, возможности `hazm` довольно сильно ограничены. Тем не менее, этот модуль может в POS-тэггинг, но для этого нужно скачать специальную [обученную модель](https://drive.google.com/file/d/1Q3JK4NVUC2t5QT63aDiVrCRBV225E_B3/edit). Это отдельный файлик, который нужно положить в ту же папку, где лежит эта тетрадка, и прописать название файла при создании объекта класса `hazm.POSTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1b7cccc-c104-4213-a944-d81474414182",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tagger = hazm.POSTagger(model=\"pos_tagger.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0686c7d-25d2-4375-ae90-4414a805edb9",
   "metadata": {},
   "source": [
    "Затем можно обратиться к тэггеру и использовать его метод `.tag()`. В него уже нужно подавать не один токен, а список токенов. В результате выдаётся список кортежей, где в каждом кортеже сначала токен, а затем список его «частеречных» тэгов.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b><i>Изучите получившийся ниже список тэгов. Все ли тэги вам понятны? Есть ли какие-то решения тэггера, которые вас удивили; решения, с которыми вы не согласны?</i></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6118ef20-84e5-4b7f-9c26-4f28402e4b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('او', 'PRON'),\n",
       " ('در', 'ADP'),\n",
       " ('کودکی', 'NOUN'),\n",
       " ('به', 'ADP'),\n",
       " ('پدر', 'NOUN,EZ'),\n",
       " ('خود', 'PRON'),\n",
       " ('کمک', 'NOUN'),\n",
       " ('می\\u200cکرد', 'VERB'),\n",
       " ('و', 'CCONJ'),\n",
       " ('غذای', 'NOUN,EZ'),\n",
       " ('فرزندان', 'NOUN,EZ'),\n",
       " ('وزیر', 'NOUN'),\n",
       " ('را', 'ADP'),\n",
       " ('برایشان', 'ADP'),\n",
       " ('به', 'ADP'),\n",
       " ('مدرسه', 'NOUN'),\n",
       " ('می\\u200cبرد', 'VERB')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468a358-fc9d-4671-b550-efbeddf2d8ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Упражнения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298c18d-ac28-4d43-9de0-c3f421df3061",
   "metadata": {},
   "source": [
    "**Упражнение 4**. Токенизируйте этот текст, найдите в нём все глаголы и выведите на экран их «основы». (Источник текста [здесь](https://fa.wikipedia.org/wiki/%D8%B3%D8%A7%D8%B1%D8%A7%D8%AA%D9%88%D9%81).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbb527-13c2-429d-bb72-33b2c9096f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"شهر ساراتوف مرکز استان ساراتوف است و از بندرهای بزرگ کناره رودخانه ولگا به‌شمار می‌آید. جمعیت این شهر در سال ۲۰۰۲ برابر با ۸۷۳٬۰۵۵ نفر بوده که بیشتر آن‌ها روس می‌باشند. اقلیت بزرگی از تاتارها، اکراینی‌ها، یهودی‌ها و آلمانی‌ها نیز در این شهر زندگی می‌کنند.\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36b2c6-4205-42e2-b8e3-72cc38c1c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваше решение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e753edf-212e-4ed5-8fd7-0900035bd9bb",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd483ca-239d-474f-b868-202f2ed41b06",
   "metadata": {},
   "source": [
    "Если вам интересна тема морфологического анализа для персидского, обратите также внимание на более тяжёлый и профессиональный модуль **`DadmaTools`**. Он предоставляет больше возможностей для этой задачи (например, выдаёт список тэгов, похожий на `pymorphy3`). А **[вот здесь](https://colab.research.google.com/drive/1-hR_ehHtTt06SMWRlM6AmV8eBYVPy_Ef?usp=sharing)** можно посмотреть на сравнение работы `hazm`, `DadmaTools` и ещё одного модуля для персидского языка. (Это была моя домашка по курсу обработки естественного языка в магистратуре.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
