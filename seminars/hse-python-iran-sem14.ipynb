{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea17432-ea86-4cb1-bed1-bdd795b70aa0",
   "metadata": {},
   "source": [
    "# Семинар 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ab864-f3a9-4dfd-8483-d90174c727f1",
   "metadata": {},
   "source": [
    "## Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab6531-7af8-455b-9b18-ec36ec45b3b0",
   "metadata": {},
   "source": [
    "**Векторизация текста**, то есть превращение текста в **векторы** (наборы чисел), — один из важнейших механизмов в NLP, который позволяет решать множество повседневных задач. Рассмотрим это явление на проблеме *выделения ключевых слов* в тексте. Мы уже пытались делать частотный словарь, но, как вы помните, самыми частотными оказывались служебные («в», «и», «это») или частотные лексические слова с абстрактным значением («сказать», «всегда»). А что если мы хотим выделить слова, которые *характеризуют* текст, сообщают что-то о его тематике? Попробуем разобраться, как это сделать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081fad0-2754-4bb5-8e87-3478cfca08ca",
   "metadata": {},
   "source": [
    "### Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e4338-3997-4cd1-b3a5-85f6387b5d1f",
   "metadata": {},
   "source": [
    "Для простоты рассмотрим этот вопрос на игрушечном корпусе текстов. Каждый из элементов списка ниже — отдельный текст. Например, первый текст — `\"арбуз\"`, а последний — `\"банан и банан и банан и ещё один банан\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc06a6d3-53a2-4be4-80d6-ad5c84db741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_corpus = [\n",
    "    \"арбуз\",\n",
    "    \"помидор и томат\",\n",
    "    \"чай и печеньки\",\n",
    "    \"сок и печеньки\",\n",
    "    \"банан и банан и банан и ещё один банан\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899771a-8faf-4a5f-98a8-08567b509896",
   "metadata": {},
   "source": [
    "Попробуем вручную (пока без кода) посчитать важность слов для конкретных текстов различными методами, постепенно усложняя вычисления.\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbe424-944c-4071-baf9-8c652525d8a5",
   "metadata": {},
   "source": [
    "Первый вариант — уже знакомый вам «частотный словарь». На самом деле не совсем корректно это называть частотным словарём, потому что мы пока что считаем не *частоту*, а просто *количество* употреблений слова в тексте. По-умному такой метод называется «**мешок слов**», или _Bag of Words_.\n",
    "- например, для текста «банан и банан и банан и ещё один банан»:\n",
    "  - $4$ употребления _банан_ в этом тексте\n",
    "  - $3$ употребление _и_ в этом тексте\n",
    "  - $1$ употребление _ещё_ в этом тексте\n",
    "  - $…$\n",
    "\n",
    "Визуализируем это в виде таблицы, где столбцы — слова, строки — тексты, а в ячейках записано число употреблений слова в тексте."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc270355-27bd-413f-853a-f64e69291a57",
   "metadata": {},
   "source": [
    "|                                            | **арбуз** | **банан** | **ещё** | **и** | **один** | **печеньки** | **помидор** | **сок** | **томат** | **чай** |\r\n",
    "|--------------------------------------------|-----------|-----------|---------|-------|----------|--------------|-------------|---------|-----------|----------|\r\n",
    "| **арбуз**                                  | 1         | 0         | 0       | 0     | 0        | 0            | 0           | 0       | 0         | 0        |\r\n",
    "| **помидор и томат**                        | 0         | 0         | 0       | 1     | 0        | 0            | 1           | 0       | 1         | 0        |\r\n",
    "| **чай и печеньки**                         | 0         | 0         | 0       | 1     | 0        | 1            | 0           | 0       | 0         | 1        |\r\n",
    "| **сок и печеньки**                         | 0         | 0         | 0       | 1     | 0        | 1            | 0           | 1       | 0         | 0        |\r\n",
    "| **банан и банан и банан и ещё один банан** | 0         | 4         | 1       | 3     | 1        | 0            | 0           | 0       | 0         | 0        |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f75fc5-4f7d-479a-8c0a-d2d39bb05988",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad677a2d-56a3-42c5-bb35-0dc459ca511b",
   "metadata": {},
   "source": [
    "Шаг второй. Попробуем учесть тот факт, что тексты могут быть разной длины. Для этого сделаем **частотный словарь**, то есть теперь уже реально посчитаем **частотность**, а не количество вхождений слова в тексте. Ещё такой процесс называется **нормализацией**, потому что после него все тексты будут иметь одинаковый вес в корпусе, несмотря на различия в размерах (сумма значений в каждом ряду будет равна $1$):\n",
    "- абсолютные значения (кол-во употреблений слова в тексте) → относительные значения (процент употреблений слова в тексте относительно других слов в тексте)\n",
    "- например, для текста «чай и печеньки»:\n",
    "  - $1$ употребление _чай_ в этом тексте $/$ всего $3$ токенов в этом тексте $→ \\frac{1}{3} ≈ 0,33$\n",
    "  - $1$ употребление _и_ в этом тексте $/$ всего $3$ токенов в этом тексте $→ \\frac{1}{3} ≈ 0,33$\n",
    "  - $1$ употребление _печеньки_ в этом тексте $/$ всего $3$ токенов в этом тексте $→ \\frac{1}{3} ≈ 0,33$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04a885-8be6-4c7b-bc4f-21fffc097ec3",
   "metadata": {},
   "source": [
    "|                                            | **арбуз** | **банан** | **ещё** | **и**  | **один** | **печеньки** | **помидор** | **сок** | **томат** | **чай** |\r\n",
    "|--------------------------------------------|-----------|-----------|---------|--------|----------|--------------|-------------|---------|-----------|----------|\r\n",
    "| **арбуз**                                  | 1         | 0         | 0       | 0      | 0        | 0            | 0           | 0       | 0         | 0        |\r\n",
    "| **помидор и томат**                        | 0         | 0         | 0       | 0.3333 | 0        | 0            | 0.3333      | 0       | 0.3333    | 0        |\r\n",
    "| **чай и печеньки**                         | 0         | 0         | 0       | 0.3333 | 0        | 0.3333       | 0           | 0       | 0         | 0.3333   |\r\n",
    "| **сок и печеньки**                         | 0         | 0         | 0       | 0.3333 | 0        | 0.3333       | 0           | 0.3333  | 0         | 0        |\r\n",
    "| **банан и банан и банан и ещё один банан** | 0         | 0.4444    | 0.1111  | 0.3333 | 0.1111   | 0            | 0           | 0       | 0         | 0        |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39286e1a-4b41-4018-9529-91888c1c40ed",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4134b865-44be-4150-b0bc-294bd3ea6411",
   "metadata": {},
   "source": [
    "Шаг третий. Надо также учесть, что некоторые слова часто встречаются во всех текстах, а другие — только в некоторых (именно они-то нам и нужны). Для этого нам нужно посчитать **частотность употреблений каждого слова во всех текстах**. Если слово употребительно во всех текстах, о каждом конкретном тексте оно ничего не сообщает — а вот если оно часто появляется только в одном или нескольких текстах, вероятно, оно отражает их уникальные особенности. (Ниже мы рассмотрим, как это сделать в питоне с помощью особого инструмента `TfidfVectorizer`.)\n",
    "\n",
    "- **стандартный вариант** (называется **TF-IDF**): разделить частотность слова в тексте на *число всех текстов*, в которых это слово встречается (_document frequency_)\n",
    "- **альтернативный вариант**: разделить частотность слова в тексте на *число вхождений этого слова во всём корпусе*\n",
    "- например, реализуем стандартный вариант для текста «чай и печеньки»:\n",
    "  - $0,33$ $/$ всего $1$ текст в корпусе, в котором встречается слово _чай_ $→ 0,33$\n",
    "  - $0,33$ $/$ всего $4$ текста в корпусе, в котором встречается слово _и_ $→ \\frac{(1/3)}{4} = \\frac{1}{12} ≈ 0,09$\n",
    "  - $0,33$ $/$ всего $2$ текста в корпусе, в котором встречается слово _печеньки_ $→ \\frac{(1/3)}{2} = \\frac{1}{6} ≈ 0,16$\n",
    "- таким образом, слово _чай_ оказывается важнее (хорошо характеризует этот текст), слово _печеньки_ чуть менее важно (хуже характеризирует этот текст), и слово _и_ совсем неважно (бессмысленно для характеристики текста)\n",
    "\n",
    "> В **`TfidfVectorizer`** используется немного математически улучшенная версия этой формулы. Числа, которые она выдаёт, слегка отличаются от приведённых выше, но главное, что слова ранжируются похожим образом: в тексте «чай и печеньки» наиболее важным оказывается слово _чай_ ($0,42$), чуть менее важным — _печеньки_ ($0,34$), совсем маловажным — союз _и_ ($0,24$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc99b7-db64-4149-b32e-4098136b7a74",
   "metadata": {},
   "source": [
    "|                                            | **арбуз** | **банан** | **ещё** | **и**  | **один** | **печеньки** | **помидор** | **сок** | **томат** | **чай** |\n",
    "|--------------------------------------------|-----------|-----------|---------|--------|----------|--------------|-------------|---------|-----------|----------|\n",
    "| **арбуз**                                  | 1         | 0         | 0       | 0      | 0        | 0            | 0           | 0       | 0         | 0        |\n",
    "| **помидор и томат**                        | 0         | 0         | 0       | 0.2198 | 0        | 0            | 0.3901      | 0       | 0.3901    | 0        |\n",
    "| **чай и печеньки**                         | 0         | 0         | 0       | 0.2377 | 0        | 0.3404       | 0           | 0       | 0         | 0.4219   |\n",
    "| **сок и печеньки**                         | 0         | 0         | 0       | 0.2377 | 0        | 0.3404       | 0           | 0.4219  | 0         | 0        |\n",
    "| **банан и банан и банан и ещё один банан** | 0         | 0.5201    | 0.13    | 0.2198 | 0.13     | 0            | 0           | 0       | 0         | 0        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906a7af-ff4f-4275-b177-0d4d2886a535",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7b122-c4f4-4f84-8c73-b0816a0444ff",
   "metadata": {},
   "source": [
    "***TF-IDF*** — известная метрика, которая позволяет оценить относительную важность слова для каждого текста в корпусе. По сути мы уже рассмотрели её выше.\n",
    "\n",
    "Рассчитывается она так:\n",
    "\n",
    "TF (*term frequency*) — **частотность слова** в тексте\n",
    "- $TF(w, t) =$ (кол-во токенов, т.е. употреблений слова $w$ в тексте $t$) $/$ (кол-во всех токенов в тексте $t$)\n",
    "\n",
    "DF (*document frequency*) — **частотность текстов**, содержащих это слово, в корпусе\n",
    "- $DF(w, T) =$ (кол-во текстов, в которых есть слово $w$) $/$ (кол-во текстов в корпусе $T$)\n",
    "- в TF-IDF используется IDF (*inverse document frequency*) — «обратная» частотность\n",
    "- рассчитывается она как $\\frac{1}{DF}$, обычно с добавлением логарифма\n",
    "- $→$ $IDF(w, T) = log(\\frac{1}{DF(w, T)})$\n",
    "  - логарифм «сглаживает» полученные числа и делает их более удобными для сравнения (это не меняет сути формулы, и понимать причину и математический смысл логарифма здесь необязательно)\n",
    "\n",
    "Итоговая формула: $TFIDF(w, t) = TF(w, t) × IDF(w, T)$.\n",
    "\n",
    "> Чем выше получившееся значение *TF-IDF*, тем более важно данное слово для конкретного текста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c53ed-8eb1-40b1-922c-43472189fc83",
   "metadata": {},
   "source": [
    "### Реализация TF-IDF в модуле **`sklearn`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca33df-3bf0-42c9-975f-ce8c0fb8cb08",
   "metadata": {},
   "source": [
    "TF-IDF можно написать с нуля, а можно использовать его реализацию в библиотеке `scikit-learn` (в питоне используется более короткое название **`sklearn`**). Это известная и широко используемая библиотека, содержащая кучу инструментов для машинного обучения. Импортируем класс `TfidfVectorizer` из подмодуля `sklearn.feature_extraction.text`, и заодно также импортируем стандартную токенизацию из `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98fc6d3-3fd7-43cf-a789-91bfbde76352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe55810-3028-49f5-b9be-a259514ada86",
   "metadata": {},
   "source": [
    "При создании объекта типа **`TfidfVectorizer`** можно не подавать ничего, а можно указать необходимые настройки (см. [подробнее здесь](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). Рассмотрим некоторые из них:\n",
    "- `tokenizer` — можно подать свою функцию для токенизации текста на токены (например, `nltk.word_tokenize`); программа будет сама токенизировать текст с помощью этой функции. Если не упоминать этот аргумент, будет просто разделение по пробелам\n",
    "- `stop_words` — можно подать свой список стоп-слов. Если не упоминать этот аргумент, фильтрации по стоп-словам не будет\n",
    "- `use_idf` — по умолчанию `True`. Если отключить (подав `False`), то IDF считаться не будет, и программа будет возвращать просто TF\n",
    "- `norm` — три опции, `None`, `\"l1\"`, `\"l2\"`\n",
    "  - если `None`, то возвращаются просто ненормализованные значения (частотный словарь с абсолютными количествами токенов)\n",
    "  - если `\"l1\"`, то происходит обычная нормализация, которую мы обсуждали выше (сумма значений для каждого текста равна 1)\n",
    "  - если `\"l1\"`, то происходит немного более сложная нормализация (сумма _квадратов_ значений для каждого текста равна 1). По некоторым математическим причинам это работает чуть лучше обычной нормализации, поэтому это значение `norm` по умолчанию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f1d12c-4936-4494-aff5-38358e989543",
   "metadata": {},
   "source": [
    "Создадим экземпляр `TfidfVectorizer`, чтобы проверить его работу на нашем игрушечном корпусе. Поставим для начала самые простые настройки: IDF не считать, нормализацию не использовать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf37752-ab2a-4ac4-ab23-eefc4eff6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, use_idf=False, norm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b3b29-cd7c-4d27-ac19-55a2fbab5285",
   "metadata": {},
   "source": [
    "Мы получили объект-векторизатор. Применим метод `.fit_transform()` — он принимает на вход корпус текстов и создаёт матрицу с числом употреблений токенов в текстах. Эта матрица хранится в специальном «сжатом» формате. Чтобы посмотреть на конкретные числа, к ней нужно применить метод `.toarray()`. Сделаем так и увидим, что мы получили нужную матрицу, которую уже подсчитали вручную выше. (Предупреждение, которое выдаёт питон, в данном случае можно смело проигнорировать.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e6c931-a834-4679-b631-0fb3c945db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samsung\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
       "       [0., 4., 1., 3., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_matrix = vectorizer.fit_transform(play_corpus)\n",
    "vector_array = vector_matrix.toarray()\n",
    "vector_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc820c4-1974-4806-9093-f5f95374c23c",
   "metadata": {},
   "source": [
    "Такую матрицу можно сделать гораздо более наглядной с помощью библиотеки **`pandas`**, которая позволяет работать с данными в табличном формате; обычно её импортируют под псевдонимом `pd`. Из матрицы можно создать объект `pd.DataFrame`, то есть датафрейм, а попросту — таблицу, в которой будут видны названия столбцов и строк. (Изучение `pandas` не входит в наши планы, поэтому разбираться в коде ячейки ниже не нужно, это просто более наглядная демонстрация для понимания работы TF-IDF.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9c948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>арбуз</th>\n",
       "      <th>банан</th>\n",
       "      <th>ещё</th>\n",
       "      <th>и</th>\n",
       "      <th>один</th>\n",
       "      <th>печеньки</th>\n",
       "      <th>помидор</th>\n",
       "      <th>сок</th>\n",
       "      <th>томат</th>\n",
       "      <th>чай</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>арбуз</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>помидор и томат</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>чай и печеньки</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>сок и печеньки</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>банан и банан и банан и ещё один банан</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        арбуз  банан  ещё    и  один  \\\n",
       "арбуз                                     1.0    0.0  0.0  0.0   0.0   \n",
       "помидор и томат                           0.0    0.0  0.0  1.0   0.0   \n",
       "чай и печеньки                            0.0    0.0  0.0  1.0   0.0   \n",
       "сок и печеньки                            0.0    0.0  0.0  1.0   0.0   \n",
       "банан и банан и банан и ещё один банан    0.0    4.0  1.0  3.0   1.0   \n",
       "\n",
       "                                        печеньки  помидор  сок  томат  чай  \n",
       "арбуз                                        0.0      0.0  0.0    0.0  0.0  \n",
       "помидор и томат                              0.0      1.0  0.0    1.0  0.0  \n",
       "чай и печеньки                               1.0      0.0  0.0    0.0  1.0  \n",
       "сок и печеньки                               1.0      0.0  1.0    0.0  0.0  \n",
       "банан и банан и банан и ещё один банан       0.0      0.0  0.0    0.0  0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus_vocabulary = vectorizer.get_feature_names_out()\n",
    "vector_table = pd.DataFrame(vector_array,\n",
    "                            columns=corpus_vocabulary,   # названия колонок — токены\n",
    "                            index=play_corpus)           # названия строк («индексы») — названия текстов\n",
    "vector_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323d705-dba3-48e0-98aa-a65d2e42a8cb",
   "metadata": {},
   "source": [
    "Теперь попробуем то же самое ещё раз, только используем дефолтные настройки `TfidfVectorizer`: пусть он нормализует данные и учитывает IDF. (Токенизацию всё же лучше оставить, потому что дефолтная токенизация, встроенная в `TfidfVectorizer`, зачем-то выкидывает из текста однобуквенные слова, а у нас тут союз «и» встречается.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc35dcbd-34eb-4efe-9cc8-d70f6e942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samsung\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>арбуз</th>\n",
       "      <th>банан</th>\n",
       "      <th>ещё</th>\n",
       "      <th>и</th>\n",
       "      <th>один</th>\n",
       "      <th>печеньки</th>\n",
       "      <th>помидор</th>\n",
       "      <th>сок</th>\n",
       "      <th>томат</th>\n",
       "      <th>чай</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>арбуз</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>помидор и томат</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>чай и печеньки</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.401565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.712775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>сок и печеньки</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.401565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.712775</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>банан и банан и банан и ещё один банан</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.875867</td>\n",
       "      <td>0.218967</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.218967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        арбуз     банан       ещё         и  \\\n",
       "арбуз                                     1.0  0.000000  0.000000  0.000000   \n",
       "помидор и томат                           0.0  0.000000  0.000000  0.370086   \n",
       "чай и печеньки                            0.0  0.000000  0.000000  0.401565   \n",
       "сок и печеньки                            0.0  0.000000  0.000000  0.401565   \n",
       "банан и банан и банан и ещё один банан    0.0  0.875867  0.218967  0.370086   \n",
       "\n",
       "                                            один  печеньки  помидор       сок  \\\n",
       "арбуз                                   0.000000  0.000000   0.0000  0.000000   \n",
       "помидор и томат                         0.000000  0.000000   0.6569  0.000000   \n",
       "чай и печеньки                          0.000000  0.575063   0.0000  0.000000   \n",
       "сок и печеньки                          0.000000  0.575063   0.0000  0.712775   \n",
       "банан и банан и банан и ещё один банан  0.218967  0.000000   0.0000  0.000000   \n",
       "\n",
       "                                         томат       чай  \n",
       "арбуз                                   0.0000  0.000000  \n",
       "помидор и томат                         0.6569  0.000000  \n",
       "чай и печеньки                          0.0000  0.712775  \n",
       "сок и печеньки                          0.0000  0.000000  \n",
       "банан и банан и банан и ещё один банан  0.0000  0.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize)\n",
    "\n",
    "vector_matrix = vectorizer.fit_transform(play_corpus)\n",
    "\n",
    "corpus_vocabulary = vectorizer.get_feature_names_out()\n",
    "vector_table = pd.DataFrame(vector_matrix.toarray(),\n",
    "                            columns=corpus_vocabulary,\n",
    "                            index=play_corpus)\n",
    "vector_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a797406-2a54-4ae3-8881-2c8c077b842b",
   "metadata": {},
   "source": [
    "До конкретных значений TF-IDF можно добраться так, как будто таблица представляет из себя словарь: сначала записать в виде «ключа» (в квадратных скобках) название столбца, а затем (в ещё одних скобках) название строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d919d12e-4cb2-478f-a936-d8cbc7d182e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "важность слова «и» по текстам\n",
      "арбуз                                     0.000000\n",
      "помидор и томат                           0.370086\n",
      "чай и печеньки                            0.401565\n",
      "сок и печеньки                            0.401565\n",
      "банан и банан и банан и ещё один банан    0.370086\n",
      "Name: и, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"важность слова «и» по текстам\")\n",
    "print(vector_table[\"и\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7efff898-491f-4e8f-bcde-c89916d34e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "важность слова «и» в тексте «банан и банан и банан и ещё один банан»\n",
      "0.3700862108940938\n"
     ]
    }
   ],
   "source": [
    "print(\"важность слова «и» в тексте «банан и банан и банан и ещё один банан»\")\n",
    "print(vector_table[\"и\"][\"банан и банан и банан и ещё один банан\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18c32c94-8b86-4678-a1b3-f69caf7d5b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "важность слова «чай» в тексте «чай и печеньки» 0.7127752157729959\n",
      "важность слова «печеньки» в тексте «чай и печеньки» 0.5750625560879445\n",
      "важность слова «и» в тексте «чай и печеньки» 0.4015651234424611\n",
      "важность слова «арбуз» в тексте «чай и печеньки» 0.0\n"
     ]
    }
   ],
   "source": [
    "for word in (\"чай\", \"печеньки\", \"и\", \"арбуз\"):\n",
    "    print(f\"важность слова «{word}» в тексте «чай и печеньки»\",\n",
    "          vector_table[word][\"чай и печеньки\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
